<!--
  Auto-generated by kmpkg tools
  --------------------------------
  This section/template was generated by kmpkg for reference.
  You may modify it freely to suit your project needs.
  Recommended to keep the structure for consistency across projects.
  version: 0.6.0
  date: 2025-12-19
-->

tally
=============================

[ä¸­æ–‡](./README_CN.md)

# tally
A **minimalist and high-performance C++ Prometheus metrics collection library** designed for large-scale clusters, built specifically for production environments with tens of thousands of machines. It prioritizes ultra-high concurrency, enables zero-intrusion integration, natively supports the Prometheus ecosystem, rejects bloated designs, and focuses solely on the core essence of metrics collection.

## ðŸš€ Core Features
### 1. Ultimate Concurrent Design: Thread-Local Write + Read-Time Aggregation, Zero Competition Overhead
Tailored for the core characteristic of monitoring metricsâ€”**write-heavy, read-light**â€”it adopts an industry-leading concurrent architecture:
- **Write Side**: Each business thread only updates its own local independent metric copy, with no locks, no atomic operation overhead, and no cache line synchronization costs. Even ten-million-level QPS high-frequency updates will not interfere with business performance.
- **Read Side**: Aggregates local data from all threads only when the `/metrics` endpoint is pulled by Prometheus. The aggregation process is non-blocking and low-latency, without affecting business logic at all.

### 2. Zero-Intrusion Integration: No Network Dependencies, No Extra Ports, Aligned with Large-Scale Enterprise O&M Standards
- **No Built-in HTTP Service**: Completely decouples the network layer, does not occupy additional port resources, and fully reuses existing business service ports and network frameworks (brpc/grpc/custom HTTP frameworks, etc.). Simply add a `/metrics` route to expose metrics.
- **Zero-Dependency & Lightweight**: Only relies on the C++ Standard Library, with no third-party dependencies. It has a small compilation footprint, can be seamlessly embedded into business processes, and requires no independent deployment.
- **Low-Coupling & Easy Access**: Features a minimalist API design. Core metrics can be defined and updated with just a few lines of code, minimizing the learning curve for developers.

### 3. Native Prometheus Ecosystem Support: Label-Based Dimension Management
- **Full Support for Prometheus Labels**: Implements multi-dimensional metric management (e.g., distinguishing service instances, interfaces, clusters) via the `Scope` Tag mechanism, with exported formats fully compliant with Prometheus standards.
- **Covers Core Metric Types**: Supports Counter (cumulative counting), Gauge (instantaneous values), and Histogram (distribution statistics), meeting the monitoring needs of most business scenarios.
- **Seamless Integration with Existing Monitoring Systems**: Prometheus Server can pull metrics directly through business ports without modifying O&M configurations such as security groups or port policies.

### 4. Production-Grade Stability: Validated in Large-Scale Clusters
- **Comprehensive Unit Tests**: Covers core logic such as metric updates, tag management, and Prometheus format export.
- **Optimized Memory Usage**: Metric objects feature a compact memory layout with no runtime dynamic memory allocation, preventing memory leaks and fragmentation.
- **Adaptable to Complex Environments**: Proven stable across tens of thousands of machines, multiple architectures, and system versions, delivering exceptional robustness.

## ðŸ†š Differentiated Advantages Over Alternatives
| Comparison Dimension       | Tally                          | Prometheus Official C++ Library (cpp-prometheus) |
|----------------------------|--------------------------------|--------------------------------------------------|
| Concurrency Design         | Thread-local write + read aggregation, zero competition | Atomic operations/locking mechanism, performance overhead under high-frequency updates |
| Network Dependencies       | No built-in HTTP, reuses business ports | Built-in HTTP service, requires additional ports |
| Integration Cost           | Zero-intrusion, embeddable in business processes with minimal code | Requires adaptation to its HTTP framework, high coupling |
| Suitability for Large-Scale Deployment | Validated in tens of thousands of machines, low O&M cost | Ideal for small clusters/testing scenarios, high O&M cost for large-scale deployment |

## ðŸ› ï¸ Build
This project uses [kmpkg](https://github.com/kumose/kmcmake) for dependency management and build integration.
kmpkg automatically handles third-party library downloads, dependency resolution, and compilation flag configuration, eliminating the need for manual maintenance of complex CMake setups.

### 0. Environment Preparation
- Linux (Ubuntu 20.04+ / CentOS 7+ recommended)
- CMake >= 3.25
- GCC >= 9.4 / Clang >= 12
- `kmpkg` installed
  (Refer to the [Installation Documentation](https://kumo-pub.github.io/docs/category/%E6%8C%81%E7%BB%AD%E9%9B%86%E6%88%90----kmpkg))

### 1. Project Configuration (Optional)
- Refer to [`kmpkg.json`](kmpkg.json) for the complete list of dependencies.
- To update the dependency baseline, modify the `baseline` field under `default-registry` in [`kmpkg-configuration.json`](kmpkg-configuration.json).
- The `baseline` value can be obtained from the latest commit via `git log`.
- Optional: Manage dependencies manually, ensuring CMake's `find_package` can locate required libraries correctly.

    - For example, install dependencies in system paths or custom directories and specify the path via `CMAKE_PREFIX_PATH`.
    - Alternatively, declare external dependency paths in kmpkg to avoid redundant downloads.

### 2. Compile the Project
Execute the following commands in the project root directory:
```bash
cmake --preset=default
cmake --build build -j$(nproc)
```

For manual dependency management:
```shell
mkdir build
cd build
cmake ..
make -j$(nproc)
```

***Note***

    Ensure the corresponding CMake Preset is defined in the project root directory when using `--preset=default`.

### 3. Run Tests (Optional)
Execute the following command in the project root directory:
```shell
ctest --test-dir build
```

## ðŸ”§ Quick Start Example
This example demonstrates Tally's core capabilities: defining multi-dimensional metrics, thread-local updates, and exposing the `/metrics` endpoint via existing business ports (aligned with real-world large-scale enterprise scenarios).

### Example Code (`example/tally_demo.cpp`)
```cpp
#include <tally/tally.h>
#include <iostream>
#include <thread>
#include <chrono>
#include <random>
#include <sstream>
// Adapt to custom enterprise HTTP frameworks; minimal pseudocode provided, can be replaced with brpc/grpc, etc.
#include "your_company_http_framework.h"

// Global metric initialization: with Prometheus Labels (Tags)
auto root_scope = tally::ScopeInstance::instance()->get_default();
// Dimension 1: Service name, Dimension 2: Port
auto service_scope = root_scope->with_tags({{"service", "order-service"}, {"port", "8080"}});

// 1. Counter: Counts total HTTP requests
tally::Counter http_req_counter;
// 2. Gauge: Tracks real-time queue length
tally::Gauge queue_length_gauge;
// 3. Histogram: Records request latency distribution (bucket boundaries: 10ms/50ms/100ms/500ms/1s)
std::vector<double> latency_buckets = {10, 50, 100, 500, 1000};
tally::Histogram req_latency_hist(latency_buckets);

void init_metrics() {
    // Expose metrics to the specified Scope (with tags)
    http_req_counter.expose("http_requests_total", "Total number of HTTP requests", service_scope.get());
    queue_length_gauge.expose("request_queue_length", "Current length of request queue", service_scope.get());
    req_latency_hist.expose("http_request_latency_ms", "Latency of HTTP requests in milliseconds", service_scope.get());
}

// Simulated business thread: updates metrics via thread-local storage (zero competition)
void business_thread(int thread_id) {
    std::random_device rd;
    std::mt19937 gen(rd() + thread_id); // Thread-specific random number generator
    std::uniform_int_distribution<> latency_dist(5, 1200); // Latency range: 5-1200ms
    std::uniform_int_distribution<> queue_dist(0, 50);     // Queue length range: 0-50

    for (int i = 0; i < 1000; ++i) {
        // 1. Update counter: thread-local write, no competition
        http_req_counter += 1;
        // 2. Update gauge: real-time queue length
        queue_length_gauge.set(queue_dist(gen));
        // 3. Record request latency to histogram
        int latency = latency_dist(gen);
        req_latency_hist.record(latency);

        // Simulate business processing
        std::this_thread::sleep_for(std::chrono::microseconds(100));
    }
    std::cout << "Thread " << thread_id << " finished, local metrics updated" << std::endl;
}

// Reuse business port for /metrics endpoint: aggregates metrics from all threads on read
void handle_metrics_request(HttpRequest& req, HttpResponse& resp) {
    // Prometheus standard response header
    resp.set_header("Content-Type", "text/plain; version=0.0.4");
    // Aggregate local metrics from all threads and export in Prometheus format
    std::ostringstream oss;
    tally::Reporter::get_prometheus_reporting(oss, nullptr);
    resp.set_body(oss.str());
}

// Simulated business endpoint: updates metrics
void handle_biz_request(HttpRequest& req, HttpResponse& resp) {
    http_req_counter += 1;
    resp.set_body("Success");
}

int main() {
    // Initialize metrics
    init_metrics();

    // 1. Start multiple business threads: verify thread-local writes
    std::vector<std::thread> threads;
    for (int i = 0; i < 4; ++i) { // Simulate 4 business threads
        threads.emplace_back(business_thread, i);
    }
    for (auto& t : threads) {
        t.join();
    }

    // 2. Reuse port 8080 for business services and register routes (real-world enterprise scenario)
    HttpServer server("0.0.0.0", 8080);
    server.register_route("/api/create_order", handle_biz_request); // Business endpoint
    server.register_route("/metrics", handle_metrics_request);      // Metrics endpoint
    std::cout << "Server running on 0.0.0.0:8080, metrics at /metrics" << std::endl;

    // Start HTTP server (blocking)
    server.run();

    return 0;
}
```

### Compile the Example
Create a new `example/CMakeLists.txt` file in the project root directory:
```cmake
# 1. Locate the Tally library (ensure Tally is installed or compiled in CMAKE_PREFIX_PATH)
find_package(tally REQUIRED CONFIG)

# 2. Optional: Define build option to control static/dynamic library linking (static by default)
option(BUILD_TALLY_STATIC "Link tally static library" ON)

# 3. Build the example executable
add_executable(tally_demo example/tally_demo.cpp)

# 4. Link the Tally library (distinguish between static and dynamic)
if(BUILD_TALLY_STATIC)
    target_link_libraries(tally_demo PRIVATE tally::tally_static)
    message(STATUS "Linking tally static library (tally_static)")
else()
    target_link_libraries(tally_demo PRIVATE tally::tally_shared)
    message(STATUS "Linking tally dynamic library (tally_shared)")
endif()

# 5. Link custom enterprise HTTP framework (if applicable, add as needed)
# target_link_libraries(tally_demo PRIVATE your_company_http_framework)

# 6. Ensure header file paths are accessible
target_include_directories(tally_demo PRIVATE ${CMAKE_CURRENT_SOURCE_DIR}/../include)
```

Compile the example:
```bash
cmake --build build --target tally_demo -j$(nproc)
```

### Running the Example
1. Execute the example program: `./build/example/tally_demo`
2. Access `http://localhost:8080/metrics` to view metrics in Prometheus format:
```
# HELP http_requests_total Total number of HTTP requests
# TYPE http_requests_total counter
http_requests_total{service="order-service",port="8080"} 4001 1734620000000

# HELP request_queue_length Current length of request queue
# TYPE request_queue_length gauge
request_queue_length{service="order-service",port="8080"} 23 1734620000000

# HELP http_request_latency_ms Latency of HTTP requests in milliseconds
# TYPE http_request_latency_ms histogram
http_request_latency_ms_sum{service="order-service",port="8080"} 245600 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="10"} 400 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="50"} 1000 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="100"} 1800 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="500"} 3600 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="1000"} 3900 0
http_request_latency_ms_bucket{service="order-service",port="8080",le="+Inf"} 4000 0
http_request_latency_ms_count{service="order-service",port="8080"} 4000 1734620000000
```

## ðŸŽ¯ Applicable Scenarios
- âœ… Basic monitoring needs for small-to-ultra-large-scale C++ clusters.
- âœ… High-concurrency services sensitive to performance, where monitoring components should not introduce additional overhead (gateways/payment systems/real-time computing).
- âœ… Production environments requiring reuse of existing business network frameworks and adherence to large-scale enterprise port planning standards.
- âœ… Embedded programs, lightweight tools, and other scenarios where independent deployment of monitoring services is not feasible.

## ðŸ“ Core Value Summary
Tally abandons the bloated "all-in-one" design of traditional monitoring libraries and returns to the essence of metrics collection. It excels in **ultra-high concurrency, zero-intrusion integration, and large-scale stability**. For teams looking to quickly embed monitoring capabilities into tens of thousands of machines without incurring additional performance or O&M costs, Tally is the ideal choice for production environments.